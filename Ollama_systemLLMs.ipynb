{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import Ollama\n",
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`CallbackManager` helps manage multiple callback handlers. It allows you to specify what should happen when certain events occur\n",
    "\n",
    "`StreamingStdOutCallbackHandler` is a specific callback handler that prints output to the standard output as the model generates text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Artificial intelligence (AI) is the ability of machines to think and act like humans.  \n",
      "\n",
      "**Think of it as:**\n",
      "\n",
      "* **Teaching computers to learn from data**: Just like you learned by experience, AI systems use massive amounts of information to improve their skills over time. \n",
      "* **Solving problems**:  AI can analyze information, identify patterns, and make decisions based on what it learns. This helps solve real-world issues in areas like healthcare, transportation, and finance.\n",
      "* **Improving our lives**: AI makes things easier and more efficient for us - think self-driving cars, personalized recommendations on Netflix, or virtual assistants that answer your questions.\n",
      "\n",
      "\n",
      "**Essentially, AI aims to mimic human intelligence in machines, allowing them to perform tasks that typically require human intelligence.** \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Artificial intelligence (AI) is the ability of machines to think and act like humans.  \\n\\n**Think of it as:**\\n\\n* **Teaching computers to learn from data**: Just like you learned by experience, AI systems use massive amounts of information to improve their skills over time. \\n* **Solving problems**:  AI can analyze information, identify patterns, and make decisions based on what it learns. This helps solve real-world issues in areas like healthcare, transportation, and finance.\\n* **Improving our lives**: AI makes things easier and more efficient for us - think self-driving cars, personalized recommendations on Netflix, or virtual assistants that answer your questions.\\n\\n\\n**Essentially, AI aims to mimic human intelligence in machines, allowing them to perform tasks that typically require human intelligence.** \\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm = Ollama(\n",
    "    model=\"gemma2:2b\", \n",
    "    callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "llm(\"Explain artificial intelligence in short\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chaining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain_llm = Ollama(\n",
    "    model=\"gemma2:2b\",\n",
    "    temperature=0.9,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jerlshin/ENV/env_torch/lib/python3.12/site-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use RunnableSequence, e.g., `prompt | llm` instead.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"topic\"],\n",
    "    template=\"Give me a short summary of {topic}\",\n",
    ")\n",
    "\n",
    "chain = LLMChain(\n",
    "    llm=chain_llm,\n",
    "    prompt=prompt,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mGive me a short summary of Artificial Intelligence\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Artificial intelligence (AI) is the ability of computer systems to mimic human intelligence, such as learning, problem-solving, and decision making. \\n\\n**Here\\'s a breakdown:**\\n\\n* **Goal:** To create machines that can perform tasks typically requiring human intelligence.\\n* **How it works:** AI uses algorithms, data sets, and computational power to learn from experiences and make predictions or decisions. \\n* **Types of AI:** There are different types:\\n    * **Machine Learning:**  Algorithms that \"learn\" from data without explicit programming.\\n    * **Deep Learning:** A subset of machine learning using artificial neural networks inspired by the human brain.\\n    * **Natural Language Processing (NLP):** Allows computers to understand, interpret, and generate human language.\\n    * **Computer Vision:** Enables computers to \"see\" and analyze images or videos.\\n\\n**Applications:** AI is revolutionizing many industries, including:\\n\\n* **Healthcare:** Diagnosing diseases, developing personalized treatments\\n* **Finance:** Detecting fraud, automating trading \\n* **Transportation:** Self-driving cars, traffic optimization\\n* **Customer service:** Personalized recommendations, chatbot interactions\\n\\n\\nAI is constantly evolving and impacting our lives in profound ways. It promises to solve complex problems and automate many tasks, but also raises ethical concerns about job displacement and bias. \\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.run(\"Artificial Intelligence\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieval Augmented Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import sys\n",
    "\n",
    "from langchain.document_loaders import WebBaseLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import GPT4AllEmbeddings\n",
    "from langchain.embeddings import OllamaEmbeddings\n",
    "\n",
    "from langchain import hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using URL: http://example.com\n"
     ]
    }
   ],
   "source": [
    "url = 'http://example.com'\n",
    "\n",
    "print(f\"using URL: {url}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = WebBaseLoader(url) # langchain.document_loaders.WebBaseLoader\n",
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split into 1 chunks\n"
     ]
    }
   ],
   "source": [
    "ollama_model = \"gemma2:2b\"\n",
    "\n",
    "# Split into chunks \n",
    "# split large documents into smaller chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1500, chunk_overlap=100)\n",
    "all_splits = text_splitter.split_documents(data)\n",
    "\n",
    "print(f\"Split into {len(all_splits)} chunks\")\n",
    "\n",
    "# creates a vectorstore to store and retrieve documents embeddings efficiently\n",
    "vectorstore = Chroma.from_documents(documents=all_splits,\n",
    "                                    embedding=OllamaEmbeddings(model=ollama_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'http://example.com', 'title': 'Example Domain', 'language': 'No language found.'}, page_content='Example Domain\\n\\n\\n\\n\\n\\n\\n\\nExample Domain\\nThis domain is for use in illustrative examples in documents. You may use this\\n    domain in literature without prior coordination or asking for permission.\\nMore information...')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1 documents\n"
     ]
    }
   ],
   "source": [
    "# Retrieve\n",
    "# question = \"What are the latest headlines on {url}?\"\n",
    "# docs = vectorstore.similarity_search(question)\n",
    "print(f\"Loaded {len(data)} documents\")\n",
    "# print(f\"Retrieved {len(docs)} documents\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "import json\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jerlshin/ENV/env_torch/lib/python3.12/site-packages/langchain/hub.py:86: DeprecationWarning: The `langchainhub sdk` is deprecated.\n",
      "Please use the `langsmith sdk` instead:\n",
      "  pip install langsmith\n",
      "Use the `pull_prompt` method.\n",
      "  res_dict = client.pull_repo(owner_repo_commit)\n"
     ]
    }
   ],
   "source": [
    "# retrieves a predefined prompt template from Langchain Hub\n",
    "QA_CHAIN_PROMPT = hub.pull(\"rlm/rag-prompt-llama\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.prompts.chat.ChatPromptTemplate'>\n"
     ]
    }
   ],
   "source": [
    "print(type(QA_CHAIN_PROMPT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!!python/object:langchain_core.prompts.chat.ChatPromptTemplate\n",
      "__dict__:\n",
      "  input_types: {}\n",
      "  input_variables:\n",
      "  - context\n",
      "  - question\n",
      "  messages:\n",
      "  - !!python/object:langchain_core.prompts.chat.HumanMessagePromptTemplate\n",
      "    __dict__:\n",
      "      additional_kwargs: {}\n",
      "      prompt: !!python/object:langchain_core.prompts.prompt.PromptTemplate\n",
      "        __dict__:\n",
      "          input_types: {}\n",
      "          input_variables:\n",
      "          - context\n",
      "          - question\n",
      "          metadata: null\n",
      "          name: null\n",
      "          optional_variables: []\n",
      "          output_parser: null\n",
      "          partial_variables: {}\n",
      "          tags: null\n",
      "          template: \"[INST]<<SYS>> You are an assistant for question-answering tasks.\\\n",
      "            \\ Use the following pieces of retrieved context to answer the question.\\\n",
      "            \\ If you don't know the answer, just say that you don't know. Use three\\\n",
      "            \\ sentences maximum and keep the answer concise.<</SYS>> \\nQuestion: {question}\\\n",
      "            \\ \\nContext: {context} \\nAnswer: [/INST]\"\n",
      "          template_format: f-string\n",
      "          validate_template: false\n",
      "        __fields_set__: !!set\n",
      "          input_variables: null\n",
      "          partial_variables: null\n",
      "          template: null\n",
      "          template_format: null\n",
      "        __private_attribute_values__: {}\n",
      "    __fields_set__: !!set\n",
      "      prompt: null\n",
      "    __private_attribute_values__: {}\n",
      "  metadata:\n",
      "    lc_hub_commit_hash: 693a2db5447e3b58c060a6ac02758dc7f1aaaaa4ee6214d127bf70b443158630\n",
      "    lc_hub_owner: rlm\n",
      "    lc_hub_repo: rag-prompt-llama\n",
      "  name: null\n",
      "  optional_variables: []\n",
      "  output_parser: null\n",
      "  partial_variables: {}\n",
      "  tags: null\n",
      "  validate_template: false\n",
      "__fields_set__: !!set\n",
      "  input_types: null\n",
      "  input_variables: null\n",
      "  messages: null\n",
      "  metadata: null\n",
      "  optional_variables: null\n",
      "  partial_variables: null\n",
      "__private_attribute_values__: {}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(yaml.dump(QA_CHAIN_PROMPT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded LLM model gemma2:2b\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jerlshin/ENV/env_torch/lib/python3.12/site-packages/langchain_core/language_models/llms.py:309: DeprecationWarning: callback_manager is deprecated. Please use callbacks instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# LLM\n",
    "llm = Ollama(model=ollama_model,\n",
    "            verbose=True,\n",
    "            callback_manager=CallbackManager([StreamingStdOutCallbackHandler()]))\n",
    "\n",
    "print(f\"Loaded LLM model {llm.model}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gemma2:2b'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jerlshin/ENV/env_torch/lib/python3.12/site-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: The method `BaseLLM.__call__` was deprecated in langchain-core 0.1.7 and will be removed in 0.3.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of India is **New Delhi**. ðŸ‡®ðŸ‡³ \n"
     ]
    }
   ],
   "source": [
    "output = llm(\"What is the capital of India?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.callbacks import FileCallbackHandler\n",
    "from langchain.callbacks import StreamlitCallbackHandler\n",
    "\n",
    "from langchain.callbacks.base import BaseCallbackHandler\n",
    "\n",
    "\n",
    "class CustomCallbackHandler(BaseCallbackHandler):\n",
    "    def on_text(self, text):\n",
    "        print(f\"Custom Output: {text}\")\n",
    "    \n",
    "callback_manager = CallbackManager([CustomCallbackHandler()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of requested results 4 is greater than number of elements in index 1, updating n_results = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This article discusses the purpose and usage of a domain named \"Example Domain.\" It provides instructions on how to utilize the domain, even without prior authorization from its creators.  It is intended for illustrative purposes in writing.  [/INST] \n"
     ]
    }
   ],
   "source": [
    "# QA chain\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "# retrieval chain Qa\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm,\n",
    "    retriever=vectorstore.as_retriever(),\n",
    "    chain_type_kwargs={\"prompt\": QA_CHAIN_PROMPT},\n",
    ")\n",
    "# Ask a question\n",
    "question = f\"What does this article talk about?\"\n",
    "result = qa_chain({\"query\": question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vectorstore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
