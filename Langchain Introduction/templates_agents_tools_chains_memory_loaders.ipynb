{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "\n",
    "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "\n",
    "text = \"what is convex optimization\"\n",
    "llm = OpenAI(temperature=0.9)\n",
    "print(llm.predict(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Convex optimization is the use of a convex function to solve a problem\n"
     ]
    }
   ],
   "source": [
    "from langchain import HuggingFaceHub\n",
    "\n",
    "text = \"what is convex optimization\"\n",
    "llm = HuggingFaceHub(repo_id=\"google/flan-t5-large\", \n",
    "                     model_kwargs={\n",
    "                         \"temperature\": 0,\n",
    "                         \"max_length\": 64 \n",
    "                     })\n",
    "response = llm.predict(text)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prompt Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is the name of first millionere man.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'abraham lincoln'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt_template = PromptTemplate(\n",
    "    input_variables=['name'],\n",
    "    template=\"What is the name of first {name} man.\"\n",
    ")\n",
    "\n",
    "p = prompt_template.format(name=\"millionere\")\n",
    "print(p)\n",
    "\n",
    "llm.predict(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Cats are domesticated cats, while dogs are free-ranging domesticated dogs.'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_template = PromptTemplate.from_template(\n",
    "    template=\"what is the difference between {cat1} and {cat2}\"\n",
    ")\n",
    "prompt = prompt_template.format(cat1=\"cat\", cat2=\"dog\")\n",
    "\n",
    "llm.predict(prompt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Men are more likely to have a family history of sexual abuse\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import LLMChain\n",
    "\n",
    "chain = LLMChain(llm=llm, prompt=prompt_template)\n",
    "response = chain.run(cat1=\"man\", cat2=\"women\")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sequential Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A burger\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import SimpleSequentialChain\n",
    "\n",
    "prompt_template_name = PromptTemplate(\n",
    "    input_variables=['cuisine'],\n",
    "    template=\"I want to open a restaurant for {cuisine} food. Suggest a fancy name for this.\"\n",
    ")\n",
    "\n",
    "prompt_template_items = PromptTemplate(\n",
    "    input_variables=['restaurant_name'],  # the output of the previous chain is the input to this chain\n",
    "    template=\"\"\"Suggest some menu items for {restaurant_name}\"\"\"\n",
    ")\n",
    "\n",
    "chain = SimpleSequentialChain(chains=[\n",
    "    LLMChain(llm=llm, prompt=prompt_template_name),\n",
    "    LLMChain(llm=llm, prompt=prompt_template_items)\n",
    "])\n",
    "\n",
    "content = chain.run('Indian')\n",
    "print(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A burger\n"
     ]
    }
   ],
   "source": [
    "print(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cuisine': 'Indian',\n",
       " 'restaurant_name': 'Indian restaurant',\n",
       " 'menu_items': 'A burger'}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Doing with KEYS for chain\n",
    "\n",
    "from langchain.chains import SequentialChain\n",
    "\n",
    "prompt_template_name = PromptTemplate(\n",
    "    input_variables=['cuisine'],\n",
    "    template=\"I want to open a restaurant for {cuisine} food. Suggest a fancy name for this.\"\n",
    ")\n",
    "name_chain =LLMChain(llm=llm, prompt=prompt_template_name, output_key=\"restaurant_name\")\n",
    "\n",
    "prompt_template_items = PromptTemplate(\n",
    "    input_variables=['restaurant_name'],  \n",
    "    template=\"\"\"Suggest some menu items for {restaurant_name}\"\"\"\n",
    ")\n",
    "food_items_chain =LLMChain(llm=llm, prompt=prompt_template_items, output_key=\"menu_items\")\n",
    "\n",
    "chain = SequentialChain(\n",
    "    chains = [name_chain, food_items_chain],\n",
    "    input_variables = ['cuisine'],\n",
    "    output_variables = ['restaurant_name', \"menu_items\"]\n",
    ")\n",
    "\n",
    "\n",
    "chain({\"cuisine\": \"Indian\"})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agents and Tools "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM making decisions about which actions to take, taking that action, seeing an Observation, and repeating that until done\n",
    "\n",
    "\"\"\"\n",
    "Tool: A function that performs a specific duty. Google search, Database lookup, Python REPL, other chains\n",
    "LLM\n",
    "Agents: agent to use\n",
    "\"\"\"\n",
    "\n",
    "# \"serpApi\" - to access google search results in real time \n",
    "\n",
    "\n",
    "# for text LLM, first we use Zero-Shot-React-Description, agent for LLMs\n",
    "# If need memory for chat model, \"Chat-Conversational-React-Description\", conversational agent\n",
    "\n",
    "\n",
    "\n",
    "os.environ['SERPAPI_API_KEY'] = '7268d43c5facc28cd61a9230ba6797b2f655e4aae51b8a948999231843b2374d'\n",
    "\n",
    "from langchain.agents import AgentType, initialize_agent, load_tools\n",
    "\n",
    "tools = load_tools([\"serpapi\", \"llm-math\"], llm=llm)\n",
    "\n",
    "\n",
    "agent = initialize_agent(\n",
    "    tools=tools,\n",
    "    llm=llm,\n",
    "    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "agent.run(\"What was the GDP of US in 2022 plus 5?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = load_tools([\"wikipedia\", \"llm-math\"], llm=llm)\n",
    "\n",
    "agent = initialize_agent(\n",
    "    tools,\n",
    "    llm,\n",
    "    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "agent.run(\"In what year was the film Departed with Leopnardo Dicaprio released? What is this year raised to the 0.43 power?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/love/ENV/env_torch/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 0.3.0. Use RunnableSequence, e.g., `prompt | llm` instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mexican restaurant\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "\n",
    "prompt_template_name = PromptTemplate(\n",
    "    input_variables =['cuisine'],\n",
    "    template = \"I want to open a restaurant for {cuisine} food. Suggest a fency name for this.\"\n",
    ")\n",
    "\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "chain = LLMChain(llm=llm,prompt=prompt_template_name)\n",
    "name = chain.run(\"Mexican\")\n",
    "print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indian restaurant\n"
     ]
    }
   ],
   "source": [
    "name = chain.run(\"Indian\")\n",
    "print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain.memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NoneType"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(chain.memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conversation Buffer Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mexican restaurant\n"
     ]
    }
   ],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "memory = ConversationBufferMemory()\n",
    "\n",
    "chain = LLMChain(llm=llm, prompt=prompt_template_name, memory=memory)\n",
    "name = chain.run(\"Mexican\")\n",
    "print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arame\n"
     ]
    }
   ],
   "source": [
    "name = chain.run(\"Arabic\")\n",
    "print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ConversationBufferMemory(chat_memory=InMemoryChatMessageHistory(messages=[HumanMessage(content='Mexican'), AIMessage(content='Mexican restaurant'), HumanMessage(content='Arabic'), AIMessage(content='arame')]))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: Mexican\n",
      "AI: Mexican restaurant\n",
      "Human: Arabic\n",
      "AI: arame\n"
     ]
    }
   ],
   "source": [
    "print(chain.memory.buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conversational Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "{history}\n",
      "Human: {input}\n",
      "AI:\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import ConversationChain  # it includes memory in built \n",
    "\n",
    "convo = ConversationChain(llm=llm)\n",
    "\n",
    "print(convo.prompt.template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The first cricket world cup was won by India.'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convo.run(\"Who won the first cricket world cup?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'5+5 is a number that is repeated indefinitely.'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convo.run(\"How much is 5+5?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The captain of the winning team was Virat Kohli.'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convo.run(\"Who was the captain ofthe winning team?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: Who won the first cricket world cup?\n",
      "AI: The first cricket world cup was won by India.\n",
      "Human: How much is 5+5?\n",
      "AI: 5+5 is a number that is repeated indefinitely.\n",
      "Human: Who was the captain ofthe winning team?\n",
      "AI: The captain of the winning team was Virat Kohli.\n"
     ]
    }
   ],
   "source": [
    "print(convo.memory.buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conversation Buffer Window Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "\n",
    "memory = ConversationBufferWindowMemory(k=3)\n",
    "\n",
    "convo = ConversationChain(\n",
    "    llm=llm,\n",
    "    memory=memory\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "convo.run(\"Who won the first cricket world cup?\")\n",
    "convo.run(\"How much is 5+5?\")\n",
    "convo.run(\"Who was the captain of the winning team?\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'India is located in South Asia.'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convo.run(\"Where is India located\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: How much is 5+5?\n",
      "AI: 5+5 is a number that is repeated indefinitely.\n",
      "Human: Who was the captain of the winning team?\n",
      "AI: The captain of the winning team was Virat Kohli.\n",
      "Human: Where is India located\n",
      "AI: India is located in South Asia.\n"
     ]
    }
   ],
   "source": [
    "print(convo.memory.buffer)   # stores only las k conv, k=3 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "loader = PyPDFLoader(\"/content/paper.pdf\")\n",
    "\n",
    "pages = loader.load()  # loades as pages "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
