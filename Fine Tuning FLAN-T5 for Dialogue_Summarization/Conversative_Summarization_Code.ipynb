{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generativ AI: **Summarize Dialogue**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForSeq2SeqLM # class for loading a using Seq2Seq language models. \n",
    "from transformers import AutoTokenizer # class for loading and using tokens\n",
    "from transformers import GenerationConfig # A class for configuring various settings related to text generation [temperature, beam_size, top-p. top-k and randomness]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "huggingface_dataset_name = \"knkarthick/dialogsum\"\n",
    "\n",
    "# 10,000 + Dataset with human labeled summaries and topics\n",
    "\n",
    "dataset = load_dataset(huggingface_dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------\n",
      "Example  1\n",
      "---------------------------------------------------------------------------------------------------\n",
      "INPUT DIALOGUE: \n",
      "#Person1#: Yeah. Just pull on this strip. Then peel off the back.\n",
      "#Person2#: You might make a few enemies this way.\n",
      "#Person1#: If they don't think this is fun, they're not meant to be our friends.\n",
      "#Person2#: You mean your friends. I think it's cruel.\n",
      "#Person1#: Yeah. But it's fun. Look at those two ugly old ladies. . . or are they men?\n",
      "#Person2#: Hurry! Get a shot!. . . Hand it over!\n",
      "#Person1#: I knew you'd come around. . .\n",
      "---------------------------------------------------------------------------------------------------\n",
      "BASELINE HUMAN SUMMARY: \n",
      "#Person1# is about to make a prank. #Person2# thinks it's cruel at first but then joins.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Example  2\n",
      "---------------------------------------------------------------------------------------------------\n",
      "INPUT DIALOGUE: \n",
      "#Person1#: Yeah. Just pull on this strip. Then peel off the back.\n",
      "#Person2#: You might make a few enemies this way.\n",
      "#Person1#: If they don't think this is fun, they're not meant to be our friends.\n",
      "#Person2#: You mean your friends. I think it's cruel.\n",
      "#Person1#: Yeah. But it's fun. Look at those two ugly old ladies. . . or are they men?\n",
      "#Person2#: Hurry! Get a shot!. . . Hand it over!\n",
      "#Person1#: I knew you'd come around. . .\n",
      "---------------------------------------------------------------------------------------------------\n",
      "BASELINE HUMAN SUMMARY: \n",
      "#Person1# is about to make a prank. #Person2# thinks it's cruel at first but then joins.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "example_indices = [50, 50]\n",
    "\n",
    "dash_line = '-'.join('' for x in range(100))\n",
    "\n",
    "for i, index in enumerate(example_indices):\n",
    "    print(dash_line)\n",
    "    print('Example ', i + 1)\n",
    "    print(dash_line)\n",
    "\n",
    "    print('INPUT DIALOGUE: ')\n",
    "    print(dataset['test'][index]['dialogue'])\n",
    "    print(dash_line)\n",
    "\n",
    "    print('BASELINE HUMAN SUMMARY: ')\n",
    "    print(dataset['test'][index]['summary'])\n",
    "    print(dash_line)\n",
    "    print()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01a5205133b2493bb4acdac1e3d8032c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.40k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf1fab49e0d04ba5aa9f3e21b8b52ed6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/990M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43f361a01bc646ad8cd79d15075b6876",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name = 'google/flan-t5-base'\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name) # download the FLAN-T% model using AutoTokenizer.from_pretrained() stuff\n",
    "# load the pretraine model, FLAN-T5 model. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16e546edbe3d4a81a189aeeebd71c570",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/2.54k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d8420b649db4bcebc4a480116f55def",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95af47cf1f274dd1bd26132ab4e58c63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/2.42M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75f2d098d3a54f569f31791ffdd65a4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/2.20k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# use_fast - switches thhe fast tokenizer. \n",
    " \n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "# use_fast = True; enables the use of the fast tokenizer implementation which is optimized fo the speed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing the Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded Sentence: \n",
      "tensor([  27,  183,  446,   49,   40,    7, 2907,  446,    5,  517,    6,  571,\n",
      "          81,  148,   58,    1])\n",
      "\n",
      "Decoded Sentence: \n",
      "I am Jerlshin J.G, How about You?\n"
     ]
    }
   ],
   "source": [
    "sentence = \"I am Jerlshin J.G, How about You?\"\n",
    "\n",
    "# tokenizes the input. \n",
    "sentence_encoded = tokenizer(sentence, return_tensors='pt')\n",
    "# return_tensors = 'pt' ; specifies that the returned result should be in PyTorch format. if 'tf' tensorflow format. \n",
    "\n",
    "\"\"\"Converts the encoded seq back into a human readable string\"\"\"\n",
    "sentence_decoded = tokenizer.decode( # convert back \n",
    "    sentence_encoded[\"input_ids\"][0], # get the input IDs from the encoded result. [0] as the batch of size 1. \n",
    "    skip_special_tokens=True # skip special tokens like [CLS], [SEP] or padding \n",
    ")\n",
    "\n",
    "print(\"Encoded Sentence: \")\n",
    "print(sentence_encoded[\"input_ids\"][0]) \n",
    "print(\"\\nDecoded Sentence: \")\n",
    "print(sentence_decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  27,  183,  446,   49,   40,    7, 2907,  446,    5,  517,    6,  571,\n",
       "           81,  148,   58,    1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  27,  183,  446,   49,   40,    7, 2907,  446,    5,  517,    6,  571,\n",
       "          81,  148,   58,    1])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_encoded[\"input_ids\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LLM Summaries wihtout Prompt Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"#Person1#: Ms. Dawson, I need you to take a dictation for me.\\n#Person2#: Yes, sir...\\n#Person1#: This should go out as an intra-office memorandum to all employees by this afternoon. Are you ready?\\n#Person2#: Yes, sir. Go ahead.\\n#Person1#: Attention all staff... Effective immediately, all office communications are restricted to email correspondence and official memos. The use of Instant Message programs by employees during working hours is strictly prohibited.\\n#Person2#: Sir, does this apply to intra-office communications only? Or will it also restrict external communications?\\n#Person1#: It should apply to all communications, not only in this office between employees, but also any outside communications.\\n#Person2#: But sir, many employees use Instant Messaging to communicate with their clients.\\n#Person1#: They will just have to change their communication methods. I don't want any - one using Instant Messaging in this office. It wastes too much time! Now, please continue with the memo. Where were we?\\n#Person2#: This applies to internal and external communications.\\n#Person1#: Yes. Any employee who persists in using Instant Messaging will first receive a warning and be placed on probation. At second offense, the employee will face termination. Any questions regarding this new policy may be directed to department heads.\\n#Person2#: Is that all?\\n#Person1#: Yes. Please get this memo typed up and distributed to all employees before 4 pm.\""
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['test'][1]['dialogue']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 1713,   345, 13515,   536,  4663,    10,   283,     7,     5, 31676,\n",
       "             6,    27,   174,    25,    12,   240,     3,     9,     3, 12194,\n",
       "           257,    21,   140,     5,  1713,   345, 13515,   357,  4663,    10,\n",
       "          2163,     6,   108,    52,   233,  1713,   345, 13515,   536,  4663,\n",
       "            10,   100,   225,   281,    91,    38,    46,  6344,    18, 19632,\n",
       "           140,  2528,   232,   440,    12,    66,  1652,    57,    48,  3742,\n",
       "             5,  1521,    25,  1065,    58,  1713,   345, 13515,   357,  4663,\n",
       "            10,  2163,     6,   108,    52,     5,  1263,  2177,     5,  1713,\n",
       "           345, 13515,   536,  4663,    10, 20748,    66,   871,   233, 18652,\n",
       "          2017,     6,    66,   828,  5030,    33, 12103,    12,   791, 17215,\n",
       "            11,  2314, 22986,     7,     5,    37,   169,    13, 18882,     3,\n",
       "         16042,  1356,    57,  1652,   383,   464,   716,    19, 15154, 18022,\n",
       "             5,  1713,   345, 13515,   357,  4663,    10,  8627,     6,   405,\n",
       "            48,  1581,    12,  6344,    18, 19632,  5030,   163,    58,   955,\n",
       "            56,    34,    92, 18395,  3866,  5030,    58,  1713,   345, 13515,\n",
       "           536,  4663,    10,    94,   225,  1581,    12,    66,  5030,     6,\n",
       "            59,   163,    16,    48,   828,   344,  1652,     6,    68,    92,\n",
       "           136,  1067,  5030,     5,  1713,   345, 13515,   357,  4663,    10,\n",
       "           299,   108,    52,     6,   186,  1652,   169, 18882,  6598,  5855,\n",
       "            12,  4521,    28,    70,   820,     5,  1713,   345, 13515,   536,\n",
       "          4663,    10,   328,    56,   131,    43,    12,   483,    70,  1901,\n",
       "          2254,     5,    27,   278,    31,    17,   241,   136,     3,    18,\n",
       "            80,   338, 18882,  6598,  5855,    16,    48,   828,     5,    94,\n",
       "          2670,     7,   396,   231,    97,    55,   852,     6,   754,   916,\n",
       "            28,     8, 22986,     5,  2840,   130,    62,    58,  1713,   345,\n",
       "         13515,   357,  4663,    10,   100,  8275,    12,  3224,    11,  3866,\n",
       "          5030,     5,  1713,   345, 13515,   536,  4663,    10,  2163,     5,\n",
       "          2372,  3490,   113, 17358,     7,    16,   338, 18882,  6598,  5855,\n",
       "            56,   166,   911,     3,     9,  6337,    11,    36,  2681,    30,\n",
       "         28444,     5,   486,   511, 10753,     6,     8,  3490,    56,   522,\n",
       "         18739,     5,  2372,   746,  1918,    48,   126,  1291,   164,    36,\n",
       "          6640,    12,  3066,  7701,     5,  1713,   345, 13515,   357,  4663,\n",
       "            10,    27,     7,    24,    66,    58,  1713,   345, 13515,   536,\n",
       "          4663,    10,  2163,     5,   863,   129,    48, 22986,   686,    26,\n",
       "            95,    11,  8308,    12,    66,  1652,   274,   314,  6366,     5,\n",
       "             1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(dataset['test'][1]['dialogue'], return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1713,   345, 13515,   536,  4663,    10,   283,     7,     5, 31676,\n",
       "             6,    27,   174,    25,    12,   240,     3,     9,     3, 12194,\n",
       "           257,    21,   140,     5,  1713,   345, 13515,   357,  4663,    10,\n",
       "          2163,     6,   108,    52,   233,  1713,   345, 13515,   536,  4663,\n",
       "            10,   100,   225,   281,    91,    38,    46,  6344,    18, 19632,\n",
       "           140,  2528,   232,   440,    12,    66,  1652,    57,    48,  3742,\n",
       "             5,  1521,    25,  1065,    58,  1713,   345, 13515,   357,  4663,\n",
       "            10,  2163,     6,   108,    52,     5,  1263,  2177,     5,  1713,\n",
       "           345, 13515,   536,  4663,    10, 20748,    66,   871,   233, 18652,\n",
       "          2017,     6,    66,   828,  5030,    33, 12103,    12,   791, 17215,\n",
       "            11,  2314, 22986,     7,     5,    37,   169,    13, 18882,     3,\n",
       "         16042,  1356,    57,  1652,   383,   464,   716,    19, 15154, 18022,\n",
       "             5,  1713,   345, 13515,   357,  4663,    10,  8627,     6,   405,\n",
       "            48,  1581,    12,  6344,    18, 19632,  5030,   163,    58,   955,\n",
       "            56,    34,    92, 18395,  3866,  5030,    58,  1713,   345, 13515,\n",
       "           536,  4663,    10,    94,   225,  1581,    12,    66,  5030,     6,\n",
       "            59,   163,    16,    48,   828,   344,  1652,     6,    68,    92,\n",
       "           136,  1067,  5030,     5,  1713,   345, 13515,   357,  4663,    10,\n",
       "           299,   108,    52,     6,   186,  1652,   169, 18882,  6598,  5855,\n",
       "            12,  4521,    28,    70,   820,     5,  1713,   345, 13515,   536,\n",
       "          4663,    10,   328,    56,   131,    43,    12,   483,    70,  1901,\n",
       "          2254,     5,    27,   278,    31,    17,   241,   136,     3,    18,\n",
       "            80,   338, 18882,  6598,  5855,    16,    48,   828,     5,    94,\n",
       "          2670,     7,   396,   231,    97,    55,   852,     6,   754,   916,\n",
       "            28,     8, 22986,     5,  2840,   130,    62,    58,  1713,   345,\n",
       "         13515,   357,  4663,    10,   100,  8275,    12,  3224,    11,  3866,\n",
       "          5030,     5,  1713,   345, 13515,   536,  4663,    10,  2163,     5,\n",
       "          2372,  3490,   113, 17358,     7,    16,   338, 18882,  6598,  5855,\n",
       "            56,   166,   911,     3,     9,  6337,    11,    36,  2681,    30,\n",
       "         28444,     5,   486,   511, 10753,     6,     8,  3490,    56,   522,\n",
       "         18739,     5,  2372,   746,  1918,    48,   126,  1291,   164,    36,\n",
       "          6640,    12,  3066,  7701,     5,  1713,   345, 13515,   357,  4663,\n",
       "            10,    27,     7,    24,    66,    58,  1713,   345, 13515,   536,\n",
       "          4663,    10,  2163,     5,   863,   129,    48, 22986,   686,    26,\n",
       "            95,    11,  8308,    12,    66,  1652,   274,   314,  6366,     5,\n",
       "             1]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(dataset['test'][1]['dialogue'], return_tensors='pt')[\"input_ids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, index in enumerate(example_indices):\n",
    "    # fetching the dialogue and summary \n",
    "    dialogue = dataset['test'][index]['dialogue']\n",
    "    summary = dataset['test'][index]['summary']\n",
    "\n",
    "    # input_ids = tokenized version of the input text , each token mapped to integer \n",
    "    # attention_mask = binary mask indicating which tokens in the input seq should be attended to and which should be ignored. \n",
    "    inputs = tokenizer(dialogue, return_tensors='pt')\n",
    "\n",
    "    # generating a summary \n",
    "    output = tokenizer.decode( # as it creates tokens, decode to the human readable text\n",
    "        model.generate( # generate the summary based on the tokenize inputs\n",
    "            inputs[\"input_ids\"],\n",
    "            max_new_tokens=50,\n",
    "        )[0],\n",
    "        skip_special_tokens=True\n",
    "    )\n",
    "\n",
    "    print(dash_line)\n",
    "    print('Example ', i + 1)\n",
    "    print(dash_line)\n",
    "\n",
    "    print(f\"INPUT PROMP:\\n{dialogue}\")\n",
    "    print(dash_line)\n",
    "\n",
    "    print(f\"BASELINE HUMAN SUMMARY:\\n{summary}\")\n",
    "    print(dash_line)\n",
    "\n",
    "    print(f\"MODEL GENERATION - WITHOUT PROMPT ENGINEERING:\\n{output}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zero shot, One shot and Few shot learning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zero shot learning with Promt Template from FLAN-T5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------\n",
      "Example  1\n",
      "---------------------------------------------------------------------------------------------------\n",
      "INPUT PROMPT:\n",
      "\n",
      "Dialogue:\n",
      "\n",
      "#Person1#: Yeah. Just pull on this strip. Then peel off the back.\n",
      "#Person2#: You might make a few enemies this way.\n",
      "#Person1#: If they don't think this is fun, they're not meant to be our friends.\n",
      "#Person2#: You mean your friends. I think it's cruel.\n",
      "#Person1#: Yeah. But it's fun. Look at those two ugly old ladies. . . or are they men?\n",
      "#Person2#: Hurry! Get a shot!. . . Hand it over!\n",
      "#Person1#: I knew you'd come around. . .\n",
      "\n",
      "What was going on?\n",
      "\n",
      "---------------------------------------------------------------------------------------------------\n",
      "BASELINE HUMAN SUMMARY:\n",
      "#Person1# is about to make a prank. #Person2# thinks it's cruel at first but then joins.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "MODEL GENERATION - ZERO SHOT:\n",
      "Person1 is going to make a few enemies.\n",
      "\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Example  2\n",
      "---------------------------------------------------------------------------------------------------\n",
      "INPUT PROMPT:\n",
      "\n",
      "Dialogue:\n",
      "\n",
      "#Person1#: Yeah. Just pull on this strip. Then peel off the back.\n",
      "#Person2#: You might make a few enemies this way.\n",
      "#Person1#: If they don't think this is fun, they're not meant to be our friends.\n",
      "#Person2#: You mean your friends. I think it's cruel.\n",
      "#Person1#: Yeah. But it's fun. Look at those two ugly old ladies. . . or are they men?\n",
      "#Person2#: Hurry! Get a shot!. . . Hand it over!\n",
      "#Person1#: I knew you'd come around. . .\n",
      "\n",
      "What was going on?\n",
      "\n",
      "---------------------------------------------------------------------------------------------------\n",
      "BASELINE HUMAN SUMMARY:\n",
      "#Person1# is about to make a prank. #Person2# thinks it's cruel at first but then joins.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "MODEL GENERATION - ZERO SHOT:\n",
      "Person1 is going to make a few enemies.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, index in enumerate(example_indices):\n",
    "    dialogue = dataset['test'][index]['dialogue']\n",
    "    summary = dataset['test'][index]['summary']\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "Dialogue:\n",
    "\n",
    "{dialogue}\n",
    "\n",
    "What was going on?\n",
    "\"\"\"\n",
    "\n",
    "    inputs = tokenizer(prompt, return_tensors='pt')\n",
    "    output = tokenizer.decode(\n",
    "        model.generate(\n",
    "            inputs[\"input_ids\"], \n",
    "            max_new_tokens=50,\n",
    "        )[0], \n",
    "        skip_special_tokens=True\n",
    "    )\n",
    "    \n",
    "    print(dash_line)\n",
    "    print('Example ', i + 1)\n",
    "    print(dash_line)\n",
    "\n",
    "    print(f'INPUT PROMPT:\\n{prompt}')\n",
    "    print(dash_line)\n",
    "\n",
    "    print(f'BASELINE HUMAN SUMMARY:\\n{summary}')\n",
    "    print(dash_line)    \n",
    "\n",
    "    print(f'MODEL GENERATION - ZERO SHOT:\\n{output}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One shot Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_prompt(example_indices_full, example_index_to_summarize):\n",
    "    prompt = ''\n",
    "    for index in example_indices_full:\n",
    "        dialogue = dataset['test'][index]['dialogue']\n",
    "        summary = dataset['test'][index]['summary']\n",
    "        \n",
    "        # The stop sequence '{summary}\\n\\n\\n' is important for FLAN-T5. Other models may have their own preferred stop sequence.\n",
    "        prompt += f\"\"\"\n",
    "Dialogue:\n",
    "\n",
    "{dialogue}\n",
    "\n",
    "What was going on?\n",
    "{summary}\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "    \n",
    "    dialogue = dataset['test'][example_index_to_summarize]['dialogue']\n",
    "    \n",
    "    prompt += f\"\"\"\n",
    "Dialogue:\n",
    "\n",
    "{dialogue}\n",
    "\n",
    "What was going on?\n",
    "\"\"\"\n",
    "        \n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dialogue:\n",
      "\n",
      "#Person1#: What time is it, Tom?\n",
      "#Person2#: Just a minute. It's ten to nine by my watch.\n",
      "#Person1#: Is it? I had no idea it was so late. I must be off now.\n",
      "#Person2#: What's the hurry?\n",
      "#Person1#: I must catch the nine-thirty train.\n",
      "#Person2#: You've plenty of time yet. The railway station is very close. It won't take more than twenty minutes to get there.\n",
      "\n",
      "What was going on?\n",
      "#Person1# is in a hurry to catch a train. Tom tells #Person1# there is plenty of time.\n",
      "\n",
      "\n",
      "\n",
      "Dialogue:\n",
      "\n",
      "#Person1#: Have you considered upgrading your system?\n",
      "#Person2#: Yes, but I'm not sure what exactly I would need.\n",
      "#Person1#: You could consider adding a painting program to your software. It would allow you to make up your own flyers and banners for advertising.\n",
      "#Person2#: That would be a definite bonus.\n",
      "#Person1#: You might also want to upgrade your hardware because it is pretty outdated now.\n",
      "#Person2#: How can we do that?\n",
      "#Person1#: You'd probably need a faster processor, to begin with. And you also need a more powerful hard disc, more memory and a faster modem. Do you have a CD-ROM drive?\n",
      "#Person2#: No.\n",
      "#Person1#: Then you might want to add a CD-ROM drive too, because most new software programs are coming out on Cds.\n",
      "#Person2#: That sounds great. Thanks.\n",
      "\n",
      "What was going on?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "example_indices_full = [40]\n",
    "example_index_to_summarize = 200\n",
    "\n",
    "\n",
    "one_shot_prompt = make_prompt(example_indices_full, example_index_to_summarize)\n",
    "\n",
    "print(one_shot_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------\n",
      "BASELINE HUMAN SUMMARY:\n",
      "#Person1# teaches #Person2# how to upgrade software and hardware in #Person2#'s system.\n",
      "\n",
      "---------------------------------------------------------------------------------------------------\n",
      "MODEL GENERATION - ONE SHOT:\n",
      "#Person1 wants to upgrade his system. #Person2 wants to add a painting program to his software. #Person1 wants to add a CD-ROM drive.\n"
     ]
    }
   ],
   "source": [
    "# ground value\n",
    "summary = dataset['test'][example_index_to_summarize]['summary']\n",
    "\n",
    "# model generated value \n",
    "inputs = tokenizer(one_shot_prompt, return_tensors='pt')\n",
    "output = tokenizer.decode(\n",
    "    model.generate(\n",
    "        inputs[\"input_ids\"],\n",
    "        max_new_tokens = 50,\n",
    "    )[0],\n",
    "    skip_special_tokens=True\n",
    ")\n",
    "\n",
    "\n",
    "print(dash_line)\n",
    "print(f'BASELINE HUMAN SUMMARY:\\n{summary}\\n')\n",
    "print(dash_line)\n",
    "print(f'MODEL GENERATION - ONE SHOT:\\n{output}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Few shot Inference "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dialogue:\n",
      "\n",
      "#Person1#: What time is it, Tom?\n",
      "#Person2#: Just a minute. It's ten to nine by my watch.\n",
      "#Person1#: Is it? I had no idea it was so late. I must be off now.\n",
      "#Person2#: What's the hurry?\n",
      "#Person1#: I must catch the nine-thirty train.\n",
      "#Person2#: You've plenty of time yet. The railway station is very close. It won't take more than twenty minutes to get there.\n",
      "\n",
      "What was going on?\n",
      "#Person1# is in a hurry to catch a train. Tom tells #Person1# there is plenty of time.\n",
      "\n",
      "\n",
      "\n",
      "Dialogue:\n",
      "\n",
      "#Person1#: May, do you mind helping me prepare for the picnic?\n",
      "#Person2#: Sure. Have you checked the weather report?\n",
      "#Person1#: Yes. It says it will be sunny all day. No sign of rain at all. This is your father's favorite sausage. Sandwiches for you and Daniel.\n",
      "#Person2#: No, thanks Mom. I'd like some toast and chicken wings.\n",
      "#Person1#: Okay. Please take some fruit salad and crackers for me.\n",
      "#Person2#: Done. Oh, don't forget to take napkins disposable plates, cups and picnic blanket.\n",
      "#Person1#: All set. May, can you help me take all these things to the living room?\n",
      "#Person2#: Yes, madam.\n",
      "#Person1#: Ask Daniel to give you a hand?\n",
      "#Person2#: No, mom, I can manage it by myself. His help just causes more trouble.\n",
      "\n",
      "What was going on?\n",
      "Mom asks May to help to prepare for the picnic and May agrees.\n",
      "\n",
      "\n",
      "\n",
      "Dialogue:\n",
      "\n",
      "#Person1#: Hello, I bought the pendant in your shop, just before. \n",
      "#Person2#: Yes. Thank you very much. \n",
      "#Person1#: Now I come back to the hotel and try to show it to my friend, the pendant is broken, I'm afraid. \n",
      "#Person2#: Oh, is it? \n",
      "#Person1#: Would you change it to a new one? \n",
      "#Person2#: Yes, certainly. You have the receipt? \n",
      "#Person1#: Yes, I do. \n",
      "#Person2#: Then would you kindly come to our shop with the receipt by 10 o'clock? We will replace it. \n",
      "#Person1#: Thank you so much. \n",
      "\n",
      "What was going on?\n",
      "#Person1# wants to change the broken pendant in #Person2#'s shop.\n",
      "\n",
      "\n",
      "\n",
      "Dialogue:\n",
      "\n",
      "#Person1#: Have you considered upgrading your system?\n",
      "#Person2#: Yes, but I'm not sure what exactly I would need.\n",
      "#Person1#: You could consider adding a painting program to your software. It would allow you to make up your own flyers and banners for advertising.\n",
      "#Person2#: That would be a definite bonus.\n",
      "#Person1#: You might also want to upgrade your hardware because it is pretty outdated now.\n",
      "#Person2#: How can we do that?\n",
      "#Person1#: You'd probably need a faster processor, to begin with. And you also need a more powerful hard disc, more memory and a faster modem. Do you have a CD-ROM drive?\n",
      "#Person2#: No.\n",
      "#Person1#: Then you might want to add a CD-ROM drive too, because most new software programs are coming out on Cds.\n",
      "#Person2#: That sounds great. Thanks.\n",
      "\n",
      "What was going on?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "example_indices_full = [40, 80, 120]\n",
    "example_index_to_summarize = 200\n",
    "\n",
    "few_shot_prompt = make_prompt(example_indices_full, example_index_to_summarize)\n",
    "\n",
    "print(few_shot_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (819 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------\n",
      "BASELINE HUMAN SUMMARY:\n",
      "#Person1# teaches #Person2# how to upgrade software and hardware in #Person2#'s system.\n",
      "\n",
      "---------------------------------------------------------------------------------------------------\n",
      "MODEL GENERATION - FEW SHOT:\n",
      "#Person1 wants to upgrade his system. #Person2 wants to add a painting program to his software. #Person1 wants to upgrade his hardware.\n"
     ]
    }
   ],
   "source": [
    "summary = dataset['test'][example_index_to_summarize]['summary']\n",
    "\n",
    "inputs = tokenizer(few_shot_prompt, return_tensors='pt')\n",
    "output = tokenizer.decode(\n",
    "    model.generate(\n",
    "        inputs[\"input_ids\"],\n",
    "        max_new_tokens=50,\n",
    "    )[0], \n",
    "    skip_special_tokens=True\n",
    ")\n",
    "\n",
    "print(dash_line)\n",
    "print(f'BASELINE HUMAN SUMMARY:\\n{summary}\\n')\n",
    "print(dash_line)\n",
    "print(f'MODEL GENERATION - FEW SHOT:\\n{output}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generative Configuration Parameters for Inference\n",
    "\n",
    "We can change the config param of the generate() method to get a difference output from the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------\n",
      "MODEL GENERATION - FEW SHOT:\n",
      "#Person1 wants to upgrade his system. #Person2 wants to add a painting program to his software. #Person1 wants to upgrade his hardware.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "BASELINE HUMAN SUMMARY:\n",
      "#Person1# teaches #Person2# how to upgrade software and hardware in #Person2#'s system.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "generation_config = GenerationConfig(max_new_tokens=50) # controlling the output. \n",
    "generation_config = GenerationConfig(max_new_tokens=50, do_sample=True, temperature=0.1) # temp controls the randomnss of the input\n",
    "\n",
    "# do_sample = True, and changing the temparture value, we get more flexibility in the output\n",
    "\n",
    "\n",
    "inputs = tokenizer(few_shot_prompt, return_tensors='pt')\n",
    "output = tokenizer.decode(\n",
    "    model.generate(\n",
    "        inputs[\"input_ids\"],\n",
    "        generation_config=generation_config,\n",
    "    )[0], \n",
    "    skip_special_tokens=True\n",
    ")\n",
    "\n",
    "print(dash_line)\n",
    "print(f'MODEL GENERATION - FEW SHOT:\\n{output}')\n",
    "print(dash_line)\n",
    "print(f'BASELINE HUMAN SUMMARY:\\n{summary}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
