{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BLIP-2 \n",
    "\n",
    "Boostrapping Language-Image Pre-training with Frozen Image Encoder and LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explain the image and answer question based on the Image\n",
    "# Using ViT and FlanT5 for these tasks - interlinked with Q-Former (Blip2)\n",
    "\n",
    "from transformers import (\n",
    "    Blip2VisionConfig,\n",
    "    Blip2VisionModel,\n",
    "    Blip2QFormerModel,\n",
    "    OPTConfig,\n",
    "    Blip2Config,\n",
    "    Blip2ForConditionalGeneration,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import requests\n",
    "from transformers import Blip2Processor\n",
    "import torch\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "\n",
    "model_to_use = \"Salesforce/blip2-opt-2.7b\"\n",
    "\n",
    "processor = Blip2Processor.from_pretrained(model_to_use)\n",
    "\n",
    "model = Blip2ForConditionalGeneration.from_pretrained(\n",
    "    model_to_use, torch_dtype=torch.float16    \n",
    ")\n",
    "model.to(device)\n",
    "url = \"http://images.cocodataset.org/val2017/000000029769.jpg\"\n",
    "image = Image.open(requests.get(url, stream=True).raw)\n",
    "\n",
    "inputs = processor(images=image, return_tensors=\"pt\").to(device, torch.float16)\n",
    "\n",
    "generated_ids = model.generate(**inputs)\n",
    "generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0].strip()\n",
    "print(generated_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With provided text prompt\n",
    "\n",
    "It can be done with or without the prompt input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Question: How many cats are there? Answer: \"\n",
    "\n",
    "inputs = processor(images=image, text=prompt, return_tensors=\"pt\").to(device, torch.float16)\n",
    "\n",
    "generated_ids = model.generate(**inputs)\n",
    "generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0].strip()\n",
    "print(prompt, generated_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoProcessor\n",
    "\n",
    "processor_auto = AutoProcessor.from_pretrained(model_to_use)\n",
    "inputs = processor_auto(image, return_tensors=\"pt\")\n",
    "generated_ids = model.generate(**inputs, max_new_tokens=50)\n",
    "generated_text_2 = processor_auto.batch_decode(generated_ids, skip_special_tokens=True)[0].strip()\n",
    "print(generated_text_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize a Blip2Config with model config\n",
    "configuration = Blip2Config()\n",
    "\n",
    "# Initialize the Blip2ForConditionalGeneration with random weights for the model\n",
    "model = Blip2ForConditionalGeneration(configuration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Blip2Config {\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"model_type\": \"blip-2\",\n",
      "  \"num_query_tokens\": 32,\n",
      "  \"qformer_config\": {\n",
      "    \"model_type\": \"blip_2_qformer\"\n",
      "  },\n",
      "  \"text_config\": {\n",
      "    \"model_type\": \"opt\"\n",
      "  },\n",
      "  \"transformers_version\": \"4.43.3\",\n",
      "  \"use_decoder_only_language_model\": true,\n",
      "  \"vision_config\": {\n",
      "    \"model_type\": \"blip_2_vision_model\"\n",
      "  }\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "configuration = model.config   # complete different Vision and Language system\n",
    "\n",
    "print(configuration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "configuration_vision = Blip2VisionConfig()\n",
    "model = Blip2VisionModel(configuration_vision)\n",
    "configuration_2 = model.config\n",
    "\n",
    "print(configuration_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
